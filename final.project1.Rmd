---
title: "Final Project"
author: "Ryan Collins and Alex Clegg"
date: "4/18/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

Here are the packages that will be needed for the project:

```{r packages}
library(readr)
library(readxl)
library(tidyverse)
library(tidymodels)
library(httr)
library(jsonlite)
library(gridExtra)
library(visdat)
library(UpSetR)
library(naniar)
library(ranger)
source("R/header.true.R")
source("R/sba_api_2012.R")
source("R/sba_api_2017.R")
source("R/sba_api_2007.R")
source("R/add_year.R")
source("R/my_theme.R")
source("R/model_CART_avg.R")
source("R/model_linear_avg.R")
source("R/model_CART_total.R")
source("R/model_linear_total.R")
```

## Project Overview:

With the coronavirus pandemic having upended life as we know it, many government from around the world have instituted massive lockdowns on economic activity to slow the spread of the virus and protect vulnerable segments of the populaiton. Unfortunately, these measures also carry a significant economic costs. In the United States alone, they have resulted in over 20 million jobs lost begining in March 2020 and caused a full scale retreat from consumption, putting many industries and businesses in serious threat of bankruptcy and insolvency. As a result, the federal government through several rounds of legislative action have tried to stem the economic losses through several programs. One - the Paycheck Protection Program (PPP) - administered by the Small Business Administration (SBA) was intented to provide businesses under 500 employees the wherewithal to withstand the immediate economic downturn with minimual loss. The original funding stipulated was $250 billion dollars.

Within the first two weeks of the program demand was overwhelming. So much so, that funding for businesses through the program ran out within a matter of weeks and forced congress to authorize an additional $310 billion dollars for the program. While the program has had challenges with scaling, access and eligiblity it has been somewhat of a success in providing businesses with a lifeline during this downturn. Moreover, as an intended policy, the program was designed to only provide funding to small buisnesses who retained the same level of payroll - stipulating that at minimum, 75% of funds must be used to provide payroll relief and to retain employees. As such, one primary goal was to keep people employed throughout the downturn such that when government-sanctioned lockdowns lifted, those workers would be able to return to a secure job instead of find themselves in line for additional unemployment benefitis. Whether or not this will be the result, it is likely too early to tell. 

There is one thing for certain - that the program experienced massive demand - such that funds were overwhelmed and institutions were forced to serve those who came first. This has led to the unnecessary prioritization of certain businesses over others when the object of the policy was to make this a guarantee for any business that might need it as a means to retain the same level of employment as they held prior to the pandemnic. Which raises the question - why didn't congress allocate what is likely a quarter of what is needed? 

While the COVID-19 pandemic is a classic black swan event in relation to other historic disaster events, SBA and federal agencies could perhaps have been better prepared to predict the specific demand needed for the program. It is the goal of this project to predict what was the average loan amount for the Paycheck Protection Program based upon previous disaster loan approvals. While this is not an apples to apples comparison, the hope is that we find a generally useful guide for loan pracitioners and understand just how much of an outlier this crisis has and continues to be.

## Data for consideration:

### Dependent Variable:

The data we will be using for this project come from multiple sources but our primary interest is in the historical SBA disaster loan data available from SBA, which cataloges the specific loss and approved loan amounts to areas impacted by natural disasters between the years of 2008 and 2018. All relevant data can be found and downloaded [here](https://www.sba.gov/about-sba/sba-performance/open-government/digital-sba/open-data/open-data-sources). 

```{r reading in SBA disaster data, echo = F, warning = F}
#first for the disaster loan-size:
sba_sandy <- read_excel("data/SBA_Disaster_Loan_Data_Superstorm_Sandy_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2008 <- read_excel("data/SBA_Disaster_Loan_Data_FY08_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2009 <- read_excel("data/SBA_Disaster_Loan_Data_FY09_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2010 <- read_excel("data/SBA_Disaster_Loan_Data_FY10_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2011 <- read_excel("data/SBA_Disaster_Loan_Data_FY11_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2012 <- read_excel("data/SBA_Disaster_Loan_Data_FY12_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2013 <- read_excel("data/SBA_Disaster_Loan_Data_FY13_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2014 <- read_excel("data/SBA_Disaster_Loan_Data_FY14_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2015 <- read_excel("data/SBA_Disaster_Loan_Data_FY15_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2016 <- read_excel("data/SBA_Disaster_Loan_Data_FY16_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2017 <- read_excel("data/SBA_Disaster_Loan_Data_FY17_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2018 <- read_excel("data/SBA_Disaster_Loan_Data_FY18_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
```


```{r adding in year variable}
#adding year variable:
years <- c(2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018)

sba_2008 <- add_year(sba_2008, 1)
sba_2009 <- add_year(sba_2009, 2)
sba_2010 <- add_year(sba_2010, 3)
sba_2011 <- add_year(sba_2011, 4)
sba_2012 <- add_year(sba_2012, 5)
sba_sandy <- add_year(sba_sandy, 5)
sba_2013 <- add_year(sba_2013, 6)
sba_2014 <- add_year(sba_2014, 7)
sba_2015 <- add_year(sba_2015, 8)
sba_2016 <- add_year(sba_2016, 9)
sba_2017 <- add_year(sba_2017, 10)
sba_2018 <- add_year(sba_2018, 11)
```

```{r mergining sba disaster loan data into single dataframe}
#binding columns:
sba_merged_all<- bind_rows(sba_sandy, sba_2008, sba_2009, sba_2010, sba_2011, sba_2012, sba_2013,
                           sba_2014, sba_2015, sba_2016, sba_2017, sba_2018)
#renaming zipcode:
sba_merged_all <- sba_merged_all %>%
  rename(zipcode = `Damaged Property Zip Code`)
```

For our dependent variable, we are looking at the specific "Approved Content Loan Amount," available within the SBA data. SBA disaster dataset includes several data sources of which approved content, we believe, to be the most comparable. Specifically, the data include verified losse and approved amounts. The losses and amounts are separated into "total", "real estate", and "content." We believe that content amount and content loss are important variables because the account for the total loss/approved amount related to the non-physical property related damages including inventory, equipment and other goods/services related loss. 

which the specific amount approved by SBA for the content a business lost in the disaster. We believe this is a useful proxy for PPP loan amounts provided they are only eligble to be used for payroll and some fixed costs. We are also looking at the SBA's Economic Injury Disaster Loan amounts as they are more comparable to the current circunstances businesses are facing with COVID.

**Conversion of Jurisdictions** 

A challenge relating to the imputation of the data from SBA has been that the data is at its most granular at the zipcode level. While we are thankful there is some inclusion of jursidiction in the data, we had hoped it would be at the county level since our predictor varaibles come from the U.S. census and are only available at that level. The challenge arises such that we need to include a further dataset to translate disaster loan amounts from zipcode to county.

```{r reading in zipcode/FIPS conversion dataframe}
#Now will wan to read in data for fips codes:
ZIP_COUNTY_FIPS_2012_12 <- read_csv("data/ZIP-COUNTY-FIPS_2012-12.csv")

#changing names for target variable (zipcode)
ZIP_COUNTY_FIPS_2012_12 <- ZIP_COUNTY_FIPS_2012_12 %>%
  rename(zipcode = ZIP) %>%
  select(zipcode, STCOUNTYFP)
```

```{r merging zipcode/FIPS to SBA disaster data}
#merging on FIPS Codes:
sba_merged_all <- sba_merged_all %>% 
  left_join(ZIP_COUNTY_FIPS_2012_12, by = "zipcode")

#now to separate out the state and county fips codes:
sba_merged_all <- sba_merged_all %>%
  mutate(stateFIPS = substr(STCOUNTYFP, 1, 2), 
         countyFIPS = substr(STCOUNTYFP, 3, 5))
```

Moreover, as anyone who's crossed this path recognizes that zipcodes change from time to time and that they are not uniform within counties - in some instances crossing boundry lines. As such, we've created severalw weighted county-total and county-averages for variables:

  - total verified content loss
  - total content approved
  - total EIDL approved

```{r creating weighted county variables}
#creating weight for countys with multiple zipcodes:
sba_agg <- sba_merged_all %>% 
  group_by(zipcode, year) %>%
  summarise(n = n())

sba_merged_all <- left_join(sba_merged_all, 
                             sba_agg,
                             by = c("zipcode", "year"))

sba_merged_all <- sba_merged_all %>%
  mutate(weighted_content_loan = `Approved Amount Content`/n,
         weighted_loss_content = `Verified Loss Content`/n,
         weighted_eidl_amount = `Approved Amount EIDL`/n) 

sba_weighted <- sba_merged_all %>%
  group_by(countyFIPS, stateFIPS, year) %>%
  summarize(
    total_county_approved_content_loan = sum(weighted_content_loan),
    total_county_verified_content_loan = sum(weighted_loss_content),
    total_county_eidl_loan = sum(weighted_eidl_amount),
    avg_county_approved_content_loan = weighted.mean(weighted_content_loan, weight = 1/n),
    avg_county_verified_content_loan = weighted.mean(weighted_loss_content, weight = 1/n),
    avg_county_eidl_loan = weighted.mean(weighted_eidl_amount, weight = 1/n)
    )
```

We weighted our variables by creating a count variable which indicated how many times a zipcode showed up by year in our merged sba_county dataset. We were then able to remerge that into our dataset and weighted our content loss, content loan, and EIDL loan by the count variable - thus splitting those amounts by the number of counties. For example that would indicate that a loan amount from a certain zipcode wholly contained in a county would not be impact, whereas a loan amount split between two counties would effectively be halfed between the two. We then took totals and averages by county and created separate variables. These will be the finalize dependent variable under consideration. 

```{r merging weighted variables onto SBA disaster loan data}
#now merging them all onto sba_merged 
sba_merged_all <- left_join(sba_merged_all, 
                             sba_weighted,
                             by = c("countyFIPS", "stateFIPS", "year"))

#choosing specific variables:
sba_merged_all <- sba_merged_all%>%
  select(`SBA Disaster Number`, `Total Approved Loan Amount`, year, zipcode, countyFIPS, stateFIPS, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan,total_county_eidl_loan, avg_county_eidl_loan)
```

### Predictor Variables: 

As mentioned earlier for our predictor variables originate from the [Economic Census](https://www.census.gov/data/developers/data-sets/economic-census.html) undertaken by the U.S. Census Bureau in five year increments (2007, 2012, 2017). According to the Census, the Economic Census is the official measure of the U.S.'s business and economy. As a survey it provides the statistical benchmark for economic activity and provides granular details on busienss size, employment numbers, sales and recipets as well as other important data. For our purposes, we are interested in number of employees, number of establishments, annual payroll, first-quarter payroll, and sales or revenue,

**Small Business Economic Census Data**

- Number of Employees: As the U.S. Treasury explains the [PPP program](https://home.treasury.gov/system/files/136/PPP--Fact-Sheet.pdf) is eligible for firms with 500 employees or less - although there is numerous instances this [might not be the case](https://www.washingtonpost.com/business/2020/05/01/sba-ppp-public-companies/). While the implementation of the program has allowed for larger firms to take advange of the program, since the goal is to provide coverage for payroll, a larger number of employees would indicate the need for larger loan sizes.

- Number of Establishments: The number of established firms in a jurisdiction influences since SBA loan performance measurea are largely [output-based](https://www.gao.gov/new.items/d08226t.pdf) as opposed to impact. As such, most SBA loan officers will likely prioritize areas with more establishments as opposed to fewers. As such, this variable is important to include. This output verus impact driven eligibility [isn't limited to only](https://crsreports.congress.gov/product/pdf/R/R43661) SBA loans.

- Annual Payroll: As mentioned above, a large goal of the PPP is to provide coverage for small buisness payroll. As such, in applying for the PPP, borrowers [must submit their previous years' annual payroll](https://www.sba.com/funding-a-business/government-small-business-loans/ppp/loan-calculator/) for underwriting of the loan. As such it's one of the most critical variables to the model.

- First Quarter Payroll: Additionally, for SBA PPP loans, monthly payroll is taken into account, such that no loan can be more than [250% of monthly payroll](https://taxfoundation.org/sba-paycheck-protection-program-cares-act/). Providing the 1st quarter payroll related to disaster may provide some clarity on what those loans would have been influenced by. This could also help to take into account some seasonanility in payroll flows.

- Sales or Revenue: This variable is important particularly for SBA content and EIDL loans where the firm needs to prove a certain amount of economic injury. [A survery](https://www.fedsmallbusiness.org/survey/2018/report-on-disaster-affected-firms) by the Federal Reserve board found that in disaster-related lending that forgone revenues (not assets) were the largest source of loss.

```{r reading and merging in Economic Census data from Census API, warning = F}
#2012 and 2007 include all the relevent NAICS data so will use for both dataframes
NAICS0712 <- c("23", "31-33", "42", "44-45", "53", "54", "56", "62", "72", "81")

sba_census_2007 <- map_df(.x = NAICS0712,
                          .f = sba_api_2007)
#Noting there is NO PAYQRT1 variable available for 2007.

#run for each year and NAICS code and the merge into three dataframes:
sba_census_2012 <- map_df(.x = NAICS0712,
                          .f = sba_api_2012)

NAICS2017 <- c("44-45", "53", "54", "56", "62", "72", "81")
#CENSUS for 2017 doesn't have Construction, Manufacturing, Whole sale retailers available.

sba_census_2017 <- map_df(.x = NAICS2017,
                          .f = sba_api_2017)

sba_census_merged <- bind_rows(sba_census_2007, sba_census_2012, sba_census_2017)
```

**North American Industry Classification System (NAICS) Limitations**

One of the challenges related to the census level data is the need to specifiy NAICS codes to avoid crashing our systems. As such, we created a function that specifies the NAICS codes for the top 10 industries who recieved PPP funding as of April 13th, 2020. Those industries are: 

- 23: Consturction 
- 31-33: Manufacturing:
- 42: Whole Sale
- 44-45:Rtail Trade
- 53: Real Estate Rental and Leasing
- 54: Professional, Scientific and Technical Services
- 56: Administrative and Support and Waste Management and Remediation Services
- 62: Health Care and Social Assistance
- 72: Accomodation and Food Services 
- 81: Other Services (expcept Public Administration)

While the rank of these industries is somewhat suspect when considering which industries are most likely to be [impacted by the crisis](https://www.brookings.edu/research/how-local-leaders-can-stave-off-a-small-business-collapse-from-covid-19/), they cover the majority of what would be likely to be impacted by the crisis. 

```{r cleaning SBA census data and averaging predictor variables}
sba_census_merged <- sba_census_merged %>%
  mutate(EMP = as.double(EMP),
         ESTAB = as.double(EMP),
         PAYANN = as.double(PAYANN),
         RCPTOT = as.double(RCPTOT),
         PAYQTR1 = as.double(PAYQTR1)) %>%
  group_by(state, county, GEO_ID, NAICS2012) %>%
  summarize(
    emp = mean(EMP),
    estab = mean(ESTAB),
    payann = mean(PAYANN),
    rcptot = mean(RCPTOT),
    payqrt1 = mean(PAYQTR1))
 
sba_census_merged <- sba_census_merged%>%
  rename(countyFIPS = county,
         stateFIPS = state)
```

We include our targeted predictor variables from the census surveys taken in 2007, 2012, and 2017 and average them such that we recieve a 10-year average for each variable. We should note that there is no first quarter payroll included in 2007 data.

```{r reading in COVID cases/deaths}
#Adding in COVID cases/deaths 5.3.20:
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")

covid <- covid %>%
  mutate(stateFIPS = substr(fips, 1, 2), 
         countyFIPS = substr(fips, 3, 5))

covid <- covid %>%
  group_by(stateFIPS, countyFIPS) %>%
  summarise(covid_deaths = sum(deaths),
            covid_cases = sum(cases))
```

To further tie the prediction to the COVID crisis we also include data provided by the New York Times on their public [GitHub page](https://github.com/nytimes/covid-19-data), which includes the count of COVID cases and deaths by county. We have to manipulate the data somewhat to total the data over time to get an accurate total per county, which we then merge with our census data.

```{r merging COVID and Census Data into single dataframe}
#merging onto sba_census data:
predictors <- sba_census_merged %>%
  left_join(covid, by = c("stateFIPS", "countyFIPS"))
```

**Michael Bay Effect**

We are also adding in an additional indicator variable to demarcate unusually significant natural disasters, which we are delineating as the top 1% of disasters. This includes storms such as Hurricane Sandy (2012), Hurricane Maria (2017), and Hurricane Harvey (2017) - all of which cause unusally high amounts of damange. All other disasters will recieve a value equal to zero. It is our hope that during the model training process we may be able to amplify the impact of a disaster, such that we can show what the cost might be were a major disaster such as Sandy, Maria, or Harvey to hit the entire country like the COVID virus has.

```{r Micheal Bay Effect Variable}
#adding column for major disaster variable:
agg_disasters <- sba_merged_all %>%
  group_by(`SBA Disaster Number`) %>%
  summarise(sum(`Total Approved Loan Amount`)) %>%
  drop_na()

names(agg_disasters)[2] <- "Total_Loans"

Baysian_Disasters <- agg_disasters %>%
  filter(Total_Loans > quantile(Total_Loans, 0.99))
#Provides the disasters in the highest 1%, which are:
#"TX-00487", "FL-00130", "NY-00130", "PR-00031", "PR-00031", "	NJ-00033"

#Adding variable for top 1% of disasters
x = c("TX-00487", "FL-00130", "NY-00130", "PR-00031", "PR-00031", "	NJ-00033")
sba_merged_all <- sba_merged_all %>%
  mutate(
MBay_effect = ifelse(sba_merged_all$`SBA Disaster Number`%in% x, 1 ,0))
```

With all that done we are then able to merge our SBA disaster loan data with our combined COVID/Census data to create a final dataframe for machine learning analysis. 

```{r merging census and SBA disaster datasets into single dataframe}
#merging county and disaster numbers:
sba_merged_final <- sba_merged_all %>% 
  left_join(predictors, by = c("countyFIPS" = "countyFIPS",
                               "stateFIPS" = "stateFIPS"))
```

### Exploring the Data Further:

Now that there is a centralized dataset including SBA disaster loan totals and economic census data from counties we will test for the relationships between the variables to help configure the model formulas we plan to use during the machine learning training and testing later. To reiterate or key depedence variables for consideration are avg/total county EDIL loans and avg/total county content loan amounts. We would like to narrow these to either EIDL or county content to minimize the number of potential models for consideration. 

First we want to check out the correlation between the variables:

**Correlation Betweeen Variables**

```{r checking out correlation, echo = F, warning = F}
#Finding correlation
sba_corr <- sba_merged_final %>%
  select(total_county_eidl_loan, avg_county_eidl_loan, total_county_approved_content_loan, avg_county_approved_content_loan, total_county_verified_content_loan, avg_county_verified_content_loan, MBay_effect, emp, payann, rcptot, payqrt1, estab, covid_cases, covid_deaths)

names_corr <- c("total_county_eidl_loan", "avg_county_eidl_loan", "total_county_approved_content_loan", "avg_county_approved_content_loan", "total_county_verified_content_loan", "avg_county_verified_content_loan", "MBay_effect", "emp", "payann", "rcptot", "payqrt1", "estab", "covid_cases", "covid_deaths")

sba_corr <- as.tibble(cor(na.omit(sba_corr)))

sba_corr <- sba_corr %>%
  mutate(names = names_corr) %>%
  select(names, total_county_eidl_loan, avg_county_eidl_loan, total_county_approved_content_loan, avg_county_approved_content_loan)
```

```{r correlations}
sba_corr
```

As we can see from the correlation output there are strong relationships with our predictor variables. At first blush it looks like our averaged depedents might have better connections than our totals. To

**Choice Between county approved content loans and EIDL loans**

As mentioned earlier - we're looking to limit our formulas to two dependent variables to limit the complexity of the models later. There are promising reasons for each variable but it would be worth checking the data plots.

```{r checking out dependent variable, echo = F}
#total_county_approved_content_loan
p1 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_approved_content_loan))) +
  geom_histogram() +
  my_theme +
  labs(x = "total content loan")

#avg_county_approved_content_loan
p2 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_approved_content_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. content loan")

#total_county_eidl_loan
p3 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "total EIDL loan")

#avg_county_eidl_loan
p4 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. EIDL loan")

```

```{r dependent grid, warning = F}
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Again as the plots indicate, the averages might prove to be a better predictor than our totals given the distribution. While we will not use a `log()` on the dependent it's clear the distributions are cleaner.

As a result - we are choosing to go with EIDL loans. This is for a couple reasons. 
1) We believe the EIDL loan mandates are more consistent with the requirements for the Paycheck Protection Program in that firms need to apply for [specific economic injury](https://www.sba.gov/funding-programs/loans/coronavirus-relief-options/economic-injury-disaster-loan-emergency-advance) related to the disaster. Similarily PPP is economic injury as opposed to physical damage. 

There is a downside for choosing EIDL loans over content. Specifically, we will likely be underestimating the total amount due to the stringency of the program compared to PPP. In the current circunstances, the SBA has loosened firm eligibility making it easier for firms to get lending and larger loans. This will be a potential liability in our model

**Exploration of Predictor Variables**

We also wanted to quickly check on the distribution of the predictor variables and whether or not there is a need to create pre-processing methods for our model fitting.

```{r checking out independent variables, echo = F}
#number of employees:
p7 <- ggplot(sba_merged_final, mapping = aes(x = log(emp))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#number of establisments:
p8 <- ggplot(sba_merged_final, mapping = aes(x = log(estab))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#annual payroll:
p9 <- ggplot(sba_merged_final, mapping = aes(x = log(payann))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#payroll in the first quarter:
p10 <- ggplot(sba_merged_final, mapping = aes(x = log(payqrt1))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#sales or revenue:
p11 <- ggplot(sba_merged_final, mapping = aes(x = log(rcptot))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#Micheal Bay effect:
p12 <- ggplot(sba_merged_final, mapping = aes(x = MBay_effect)) +
  geom_histogram(binwidth = 0.35) +
  my_theme
```

```{r independent grid, warning = F}
grid.arrange(p7, p8, p9, p10, p11, p12, nrow = 2)
```

All the plots created above included `log()` transformations. It is clear we will need additional preprocessing steps to also include this transformation. 

## Model specifications:

**Algorithm Considerations: **

We think it would be good to have a couple options to help see which might help the model fit better. Specifically:

- CART Decision Tree: We believe this will likely be our best algorithm for the complexity of the models we are using. We also think it provides the best ease of understanding for our audience. While there are many versions of tree-based algorithms, we think the [Classification and Regression Tree (CART)](https://bradleyboehmke.github.io/HOML/DT.html), makes the most sense since many of our variables are numeric and discreate. Since we are also using total and averaged loan sizes across county, the basic function of how trees create subgroups via an averaged response should make the algorithm more accurate for our dataset. 

- Linear Regression: We also felt it was important to include a baseline algorithm like linear regression. First, it's easy to understand when explaining our findings. That said, there are likely downsides to this model. It is likely our data is not linear in any form. Moroever, we would likely need to incldue further dummy variables to account for the non-linearity. Nonetheless, we think it's important to include if for no other reason than an easily-explainable baseline.

- Random Forests: We also wanted to explore a more robust version of decision trees, with a random forest algorithm. We understand this algorithym should be [easy to use out-of-the-box](https://bradleyboehmke.github.io/HOML/random-forest.html), but we need to further understand some of our missing values and their releationship to the data since the algorithm won't work with missing values.

**Testing Two Models**

After exploring the data further we have a two diffeferent potential model formulas to test:

- Average County EIDL Loan as a function of: average county verified content loan, employees, annual payroll, total recepits, payroll in the first quarter, number of establishments in its jursdiction, cases of COVID-19, and deaths from COVID-19. We are assuming that this will be a top 1% disaster, triggering the Michael Bay effect. [avg_county_eidl_loan ~ avg_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect]

- Total County EIDL Loan as a function of: total county verified content loan, employees, annual payroll, total recepits, payroll in the first quarter, number of establishments in its jursdiction, cases of COVID-19, and deaths from COVID-19. We are assuming that this will be a top 1% disaster, triggering the Michael Bay effect. [total_county_eidl_loan ~ total_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect]

**Missing Values**

As mentioned earlier, it is important to consider the missing values in our data set. This is important since the random forest algorithm will require it but also how NAs will imapct on our other models. We will quickly subset our data to the variables of interest and create a sample of 50,000 observations so running tests is managable. 

```{r checking on missing values}
#to get a subset for missing visualizations
sba_merged_model <- sba_merged_final %>%
  select(avg_county_eidl_loan, total_county_eidl_loan, total_county_verified_content_loan, 
         payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  sample_n(50000) 
```

```{r visualizing missing data across variables}
vis_miss(sba_merged_model) #to see NAs across the variable spectrum
```

It looks like we only have significant missing values in our first quarter payroll as well as in our covid cases and covid deaths.

```{r visualizing relationship between missing values}
gg_miss_upset(sba_merged_model) #checking about relationship between the variables
```

As the earlier graph indicated a lot of the data's missing values are within the covid-related data and the payroll for the first quarter. While there is some relationship across the missing values we feel confident it isn't going to grealy impact the estimations of the models nor remove incredibly useful information from our estimations. 

### Pre-Processing: 

- formatting dependent variables:
  * As mentioend earlier most of our dependent variable will be transformed with a `step_log()` command. these include our number of employees, annual payroll, number of estabilishment, payroll for the first quarter, and the firms revenue and sales. COVID cases and deaths as well as our Michael Bay factor will not be transormed. We have also included a command to offset the values due numerous observations in all variables equal to zero.
  
```{r pre-process for models}
#pre-process for CART and linear:
sba_merged_model <- sba_merged_final %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, avg_county_approved_content_loan, total_county_approved_content_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(sba_merged_model) %>% #omitting missing values
  sample_n(50000) #subsetting the dataset for training and testing

set.seed(seed = 20200428)

split <- initial_split(sba_merged_model, prop = 0.80)
SBA_train <- training(split)
SBA_test <- testing(split)
SBA_resamples <- vfold_cv(data = SBA_train,
                          v = 10)
```

### Error Term:

When comparing models, we elected to use Root Mean Squared Error (RMSE) to estimate model error. RMSE was chosen because of its flexibility for both CART and linear regression. RMSE also weighs outliers more heavily, and since this economic criss is, by defintion, an outlier - we think it is important to base our model's estimations that incorporate outlier observations.

### Testing Models in Resampling for CART and Linear algorithms:

We will be testing four specific models for each of the two formulas and each of the two algorithms such that we will have:
- Averaged CART
- Total CART
- Averaged Linear
- Total Linear 

```{r linear and CART regression algorithm, warning = F}
#Comparing RMSE across our two formulas and two algorithms:
SBA_resamples <- SBA_resamples %>%
  mutate(
    rmse_CARTavg = map_dbl(.x = splits,
                         .f = ~model_CART_avg(split = .x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_linearavg = map_dbl(.x = splits,
                           .f = ~model_linear_avg(.x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_CARTtotal = map_dbl(.x = splits,
                         .f = ~model_CART_total(split = .x,
                                          formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                            emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                            covid_deaths + MBay_effect)),
    rmse_lineartotal = map_dbl(.x = splits,
                           .f = ~model_linear_total(.x,
                                              formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                                emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                                covid_deaths + MBay_effect)))
```

We will first want to look across the ten folds:
```{r across resamples}
SBA_resamples %>%
  select(id, rmse_CARTavg, rmse_linearavg, rmse_CARTtotal, rmse_lineartotal)
```

As well as look at the mean of the resamples:
```{r mean of resamples}
SBA_resamples %>%
  summarize(mean(rmse_linearavg),
            mean(rmse_CARTavg),
            mean(rmse_CARTtotal),
            mean(rmse_lineartotal))
```

It's clear from this model that our **Averaged CART** does the best job among the four mdoel options. This tracks with our intuition: this is a complex model with many variables and is more naturally sutied to a decision tree model than a simple linear model. Taking this logic one step further we attempted to run a random forests model to further reduce RMSE. 

**Random Forest Test**

``` {r setting seed and splitting into training and testing}
#pre-process:
set.seed(seed = 20200428)

sba_merged_model_rf <- sba_merged_final %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(sba_merged_model_rf) %>%
  sample_n(50000)

split_rf <- initial_split(sba_merged_model_rf, prop = 0.80)
SBA_train_rf <- training(split_rf)
SBA_test_rf <- testing(split_rf)
```

```{r RandomForest algorithm for avg_county_eidl_loan}
n_features <- length(setdiff(names(SBA_train_rf), "avg_county_eidl_loan"))

sba_rf_avg <- ranger(
  formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train_rf,
  num.tree = n_features*10,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(avg_rmse <- sqrt(sba_rf_avg$prediction.error))
```

We can see that across the training set, that our averaged random forest does better than most.

```{r RandomForest algorithm for total_county_eidl_loan}
n_features1 <- length(setdiff(names(SBA_train_rf), "total_county_eidl_loan"))

sba_rf_total <- ranger(
  formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train_rf,
  num.tree = n_features1*10,
  mtry = floor(n_features1 / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(total_rmse <- sqrt(sba_rf_total$prediction.error))
```

It also appears that random forest for total does better compared to other totals across the training data.

The random forests model decreased the RMSE and produced a better model than CART. However, due to our computational limitations, we were not able to explore random forests further as part of this analysis.

Hence, we elected to go with average county EIDL loan via CART as our preferred final model for testing.

``` {r Final Model}
#remove NA:
SBA_train <- SBA_train %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, avg_county_approved_content_loan, total_county_approved_content_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(SBA_train)

#create recipe:
final_recipe<- 
    recipe(formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
           data = SBA_train) %>%
    step_log(total_county_verified_content_loan, offset = 0.00001) %>%
    step_log(emp, offset = 0.00001) %>%
    step_log(payann, offset = 0.00001) %>%
    step_log(rcptot, offset = 0.00001) %>%
    step_log(payqrt1, offset = 0.00001) %>% 
    step_log(estab, offset = 0.00001) %>% 
    prep()

#create model:
final_model <- decision_tree(mode = "regression") %>%
    set_engine("rpart") %>%
    fit(formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect, 
    data = SBA_train)

#test model:
final_rmse <- bind_cols(
  bake(final_recipe, new_data = SBA_test),
  pred = predict(final_model, bake(final_recipe, new_data = SBA_test)) 
  )%>%
  rmse(truth = avg_county_eidl_loan, estimate = .pred) %>%
  pull(.estimate)


final_rmse

```

As we can see from the final output there has been a slight uptick in the RMSE when running our model through our testing data. Our estimate becomes approximately $12,000 less accurate in predicting what EIDL loan sizes would be from natural disasters. We believe this isn't a terrible conclusion but believe there are areas of improvement, which we will discuss later.

**Bias - Variance Tradeoff**
We believe it is important to note at this point that our current model is likley of low-bias and high variance, which is probably an okay place to arrive given the algorithm of choice. Again, we will discuss areas of improvement later.

## Applying Model to Current Crisis:

**Unfortunately Don't Have Current Data:**

As of this writing, the SBA has yet to release data related to the PPP. Once that data is available, we can compare that against our model to see how effetive it would have been.

**Still Upsides to the Model**

Having a model like this available to policymakers as they craft policy with as much as uncertainty as PPP would enable better policy outcomes and proper funding of programs, eliminating situtations similar to the one faced at the outset of this program. Moreover, we believe the creation of the Michael Bay effect could help to engingeer a stronger prediction in future data, since we can in short tweak the variable to anticipate how much of an outlier the coronavirus pandemic has been. In short, we could try to predict what loan sizes would be had Harvey, Maria, or Sandy hit the entire country as opposed to specific states.

## Areas of Improvement:

We believe the first attempt at the model was sufficient but as noted there are areas where it can be great improved. First would be to consider a more aggegated variable for prediction. While EIDL were more aligned by need, as a loan, underestimate the need and account for a smaller portion of natural disaster funding. Content Loans could have perhaps proven a better fit and would be the first place to start given the Federal Reserve has indicated that [40% of 7(a) SBA disaster funding](https://www.fedsmallbusiness.org/survey/2018/report-on-disaster-affected-firms) is accounted by revenue loss and not physical property damage. 

Moreover, there are likely errors in the data wrangling which could further account for less accurate data. With more time, accounting for those errors would make for a more accurate model. The same goes for handling missing information. Instead of ommitting the data, we could go back and imput the missing data with machine learning models, although that would require significant more time and thought. 

Lastly, related to our attempt to balance or model's bias - variance trade off, we could further understand other algorithm options - like we tried with random forests - and further tune and dial the paremeters to understand the optimal arrangement for nodes, and subgroups in our decision tree models and other counfounding variables in our linear regression - we are sure we are missing some dummy variables that are important. It is important to note, that while we recognize the need for further balancing, given the implicit challenge at predicting current data on historical that balancing challenge will always exisit.

Nonetheless, we feel proud of the work presented here and hope that the SBA will release PPP data soon, thus allowing us to really see how well our model performed.
