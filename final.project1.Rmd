---
title: "Final Project"
author: "Ryan Collins and Alex Clegg"
date: "4/18/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

Here are the packages that will be needed for the project:

```{r packages}
library(readr)
library(readxl)
library(tidyverse)
library(tidymodels)
library(httr)
library(jsonlite)
library(gridExtra)
library(visdat)
library(UpSetR)
library(naniar)
library(ranger)
source("R/header.true.R")
source("R/sba_api_2012.R")
source("R/sba_api_2017.R")
source("R/sba_api_2007.R")
source("R/add_year.R")
source("R/my_theme.R")
source("R/model_CART_avg.R")
source("R/model_linear_avg.R")
source("R/model_CART_total.R")
source("R/model_linear_total.R")
```

## Project Overview

With the coronavirus pandemic having upended life as we know it, many governments from around the world have instituted massive lockdowns on economic activity to slow the spread of the virus and protect vulnerable segments of the population. Unfortunately, these measures also carry a significant economic cost. In the United States alone, they have resulted in [26 million jobs losses](https://www.npr.org/sections/coronavirus-live-updates/2020/04/23/841876464/26-million-jobs-lost-in-just-5-weeks) since March and umemployment rates predicted at almost 20%. This has forced many households into a full-scale retreat from consumption and put many industries and businesses in serious threat of [bankruptcy and insolvency](https://www.nytimes.com/2020/05/03/business/j-crew-bankruptcy-coronavirus.html). As a result, the federal government through several rounds of legislative action have tried to stem the economic losses through several lending programs. One - the [Paycheck Protection Program (PPP)](https://www.sba.gov/funding-programs/loans/coronavirus-relief-options/paycheck-protection-program) - administered by the Small Business Administration (SBA) was intended to provide businesses under 500 employees the wherewithal to withstand the immediate economic downturn with minimal loss. The original funding stipulated was [$250 billion dollars](https://www.congress.gov/bill/116th-congress/senate-bill/3548/text?q=product+update).

Within the first two weeks of the program [demand was overwhelming](https://www.rollcall.com/2020/04/09/congress-works-on-sba-aid-bill-amid-overwhelming-demand/). So much so, that funding for businesses ran out within a matter of weeks and forced congress to authorize an additional $310 billion dollars for the program. While the program has [faced challenges](https://www.npr.org/2020/05/04/848389343/how-did-the-small-business-loan-program-have-so-many-problems-in-just-4-weeks) with scaling, access and eligibility it has helped provide businesses with a lifeline during this downturn. Moreover, as an intended policy, the program was designed to only provide funding to small businesses who retained the [same level of payroll](https://crsreports.congress.gov/product/pdf/IN/IN11329) - stipulating that at minimum, 75% of funds must be used to provide payroll relief and to retain employees. As such, one primary goal was to keep people employed throughout the downturn such that when government-sanctioned lockdowns lifted, those workers would be able to return to a secure job instead of finding themselves in line for additional unemployment benefits. Whether or not this will be the result, it is too early to tell. 

There is one thing for certain - that the program experienced massive demand - such that banks were overwhelmed, and institutions were forced [to serve those who came first](https://www.chicagotribune.com/coronavirus/ct-nw-nyt-banks-special-treatment-ppp-loans-20200423-pn56tsaz6fhrhg7xppvokphmiq-story.html). This has led to the unnecessary prioritization of certain businesses over others when the object of the policy was to make this a guarantee for any business that might need it as a means to retain the same level of employment as they held prior to the pandemic. Which raises the question - why didn't congress allocate what is likely a quarter of what is needed? 

While the COVID-19 pandemic is a classic black swan event in relation to other historic disaster events, SBA and federal agencies could perhaps have been better prepared to predict the specific demand needed for the program. It is the goal of this project to predict what was the average loan amount for the Paycheck Protection Program should be based upon previous disaster loan approvals. While this is not an apples to apples comparison, the hope is that we find a generally useful future guide for loan practitioners and understand just how much of an outlier this crisis has and continues to be.

## Data for consideration

### Dependent Variable:

The data we will be using for this project come from multiple sources, but our primary interest is in the historical SBA disaster loan data available from SBA, which catalogs the specific loss and approved loan amounts to areas impacted by natural disasters between the years of 2008 and 2018. All relevant data can be found and downloaded [here](https://www.sba.gov/about-sba/sba-performance/open-government/digital-sba/open-data/open-data-sources).


```{r reading in SBA disaster data, echo = F, warning = F}
#first for the disaster loan-size:
sba_sandy <- read_excel("data/SBA_Disaster_Loan_Data_Superstorm_Sandy_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2008 <- read_excel("data/SBA_Disaster_Loan_Data_FY08_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2009 <- read_excel("data/SBA_Disaster_Loan_Data_FY09_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2010 <- read_excel("data/SBA_Disaster_Loan_Data_FY10_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2011 <- read_excel("data/SBA_Disaster_Loan_Data_FY11_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2012 <- read_excel("data/SBA_Disaster_Loan_Data_FY12_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2013 <- read_excel("data/SBA_Disaster_Loan_Data_FY13_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2014 <- read_excel("data/SBA_Disaster_Loan_Data_FY14_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2015 <- read_excel("data/SBA_Disaster_Loan_Data_FY15_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2016 <- read_excel("data/SBA_Disaster_Loan_Data_FY16_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2017 <- read_excel("data/SBA_Disaster_Loan_Data_FY17_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2018 <- read_excel("data/SBA_Disaster_Loan_Data_FY18_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
```

```{r adding in year variable, echo = F}
#adding year variable for weighted computation laterr:
years <- c(2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018)

sba_2008 <- add_year(sba_2008, 1)
sba_2009 <- add_year(sba_2009, 2)
sba_2010 <- add_year(sba_2010, 3)
sba_2011 <- add_year(sba_2011, 4)
sba_2012 <- add_year(sba_2012, 5)
sba_sandy <- add_year(sba_sandy, 5)
sba_2013 <- add_year(sba_2013, 6)
sba_2014 <- add_year(sba_2014, 7)
sba_2015 <- add_year(sba_2015, 8)
sba_2016 <- add_year(sba_2016, 9)
sba_2017 <- add_year(sba_2017, 10)
sba_2018 <- add_year(sba_2018, 11)
```

```{r mergining sba disaster loan data into single dataframe}
#binding columns:
sba_merged_all<- bind_rows(sba_sandy, sba_2008, sba_2009, sba_2010, sba_2011, sba_2012, sba_2013,
                           sba_2014, sba_2015, sba_2016, sba_2017, sba_2018)
#renaming zipcode:
sba_merged_all <- sba_merged_all %>%
  rename(zipcode = `Damaged Property Zip Code`)
```

For our dependent variable, we are looking at either the specific "Approved Content Loan Amount," or the "EIDL loans" available within the SBA data. 

SBA disaster data includes several data variables of which EIDL or approved content, we believe, to be the most comparable to the current circunstances. Generally, the data include verified losses and approved loan amounts. The loss and loan amounts are separated into "total", "real estate", and "content." We believe that the content amount and content loss are important variables because the account for the total loss/approved amount related to non-physical property related damages including inventory, equipment and other goods/services related losses. Additionally, EIDL loans are specifically geard to cover economic loss that is not phsicial in nature. As such, we believe these to be useful proxies for current PPP loan amounts.

**Conversion of Jurisdictions** 


A challenge relating to the creation of this dataset is that data from SBA are most granular at the zip code level. While we are thankful there is some inclusion of jurisdiction in the data, we had hoped it would be at the county level since our predictor variables come from the U.S. census and are only available at that level. The challenge arises from this mismatch is that we need to include a further dataset to translate disaster loan amounts from zip code to county.

ster
```{r reading in zipcode/FIPS conversion dataframe}
#Now will wan to read in data for fips codes:
ZIP_COUNTY_FIPS_2012_12 <- read_csv("data/ZIP-COUNTY-FIPS_2012-12.csv")

#changing names for target variable (zipcode)
ZIP_COUNTY_FIPS_2012_12 <- ZIP_COUNTY_FIPS_2012_12 %>%
  rename(zipcode = ZIP) %>%
  select(zipcode, STCOUNTYFP)
```

```{r merging zipcode/FIPS to SBA disaster data}
#merging on FIPS Codes:
sba_merged_all <- sba_merged_all %>% 
  left_join(ZIP_COUNTY_FIPS_2012_12, by = "zipcode")

#now to separate out the state and county fips codes:
sba_merged_all <- sba_merged_all %>%
  mutate(stateFIPS = substr(STCOUNTYFP, 1, 2), 
         countyFIPS = substr(STCOUNTYFP, 3, 5))
```

Moreover, as anyone who's crossed this path recognizes, zip codes change from time to time and they are not uniform within counties - in that - some zip codes cross county boundary lines. As such, we will need to create several weighted county-total and county-averages for our SBA loan amount variables.

We've created averages and totals for three variables with the code below:

  - verified content loss
  - content loan approved
  - EIDL loans approved

```{r creating weighted county variables}
#creating weight for countys with multiple zipcodes:
sba_agg <- sba_merged_all %>% 
  group_by(zipcode, year) %>%
  summarise(n = n())

sba_merged_all <- left_join(sba_merged_all, 
                             sba_agg,
                             by = c("zipcode", "year"))

sba_merged_all <- sba_merged_all %>%
  mutate(weighted_content_loan = `Approved Amount Content`/n,
         weighted_loss_content = `Verified Loss Content`/n,
         weighted_eidl_amount = `Approved Amount EIDL`/n) 

sba_weighted <- sba_merged_all %>%
  group_by(countyFIPS, stateFIPS, year) %>%
  summarize(
    total_county_approved_content_loan = sum(weighted_content_loan),
    total_county_verified_content_loan = sum(weighted_loss_content),
    total_county_eidl_loan = sum(weighted_eidl_amount),
    avg_county_approved_content_loan = weighted.mean(weighted_content_loan, weight = 1/n),
    avg_county_verified_content_loan = weighted.mean(weighted_loss_content, weight = 1/n),
    avg_county_eidl_loan = weighted.mean(weighted_eidl_amount, weight = 1/n)
    )
```

As mentioned, we weighted our variables by creating a count of how many times a zip code showed up by year and county. We were then weighted our content loss, content loan, and EIDL loan by the count variable - dividing by `n()` for totals and $1/n()$ for averages - thus splitting those loan amounts by the number of counties they show up in. That would indicate that a loan amount from a certain zip code wholly contained in a county would not be impacted, whereas a loan amount split between two counties would effectively be halved between the two. We then took totals and averages by county and created separate variables. 

- total_county_approved_content_loan
- avg_county_approved_content_loan
- total_county_eidl_loan
- avg_county_eidl_loan

These will be the finalize dependent variable under consideration for our model.

```{r merging weighted variables onto SBA disaster loan data}
#now merging them all onto sba_merged 
sba_merged_all <- left_join(sba_merged_all, 
                             sba_weighted,
                             by = c("countyFIPS", "stateFIPS", "year"))

#choosing specific variables:
sba_merged_all <- sba_merged_all%>%
  select(`SBA Disaster Number`, `Total Approved Loan Amount`, year, zipcode, countyFIPS, stateFIPS, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan,total_county_eidl_loan, avg_county_eidl_loan)
```

### Predictor Variables: 

As mentioned earlier for our predictor variables originate from the [Economic Census](https://www.census.gov/data/developers/data-sets/economic-census.html) undertaken by the U.S. Census Bureau in five year increments (2007, 2012, 2017). According to the Census, the Economic Census is the official measure of the U.S.'s business and economy. As a survey it provides the statistical benchmark for economic activity and provides granular details on business size, employment numbers, sales and revenue as well as other important data. For our purposes, we are interested in number of employees, number of establishments, annual payroll, first-quarter payroll, and sales or revenue,

**Small Business Economic Census Data**

- Number of Employees: As the U.S. Treasury explains the [PPP program](https://home.treasury.gov/system/files/136/PPP--Fact-Sheet.pdf) is eligible for firms with 500 employees or less - although there is numerous instances this [might not be the case](https://www.washingtonpost.com/business/2020/05/01/sba-ppp-public-companies/). While the implementation of the program has allowed for larger firms to take advantage of the program, since the goal is to provide coverage for payroll, a larger number of employees would indicate the need for larger loan sizes.

- Number of Establishments: The number of established firms in a jurisdiction influences the loan size since SBA loan performance measures are largely [output-based](https://www.gao.gov/new.items/d08226t.pdf) as opposed to outcome. As such, most SBA loan officers will likely prioritize areas with more establishments as opposed to fewer. As such, this variable is important to include. This output versus impact driven eligibility [isn't limited to only](https://crsreports.congress.gov/product/pdf/R/R43661) SBA loans.

- Annual Payroll: As mentioned above, a large goal of the PPP is to provide coverage for small business payroll. As such, in applying for the PPP, borrowers [must submit their previous years' annual payroll](https://www.sba.com/funding-a-business/government-small-business-loans/ppp/loan-calculator/) for underwriting of the loan. Therefore we think it's important variable to include.

- First Quarter Payroll: Additionally, for SBA PPP loans, monthly payroll is taken into account, such that no loan can be more than [250% of previous month's cost](https://taxfoundation.org/sba-paycheck-protection-program-cares-act/). Providing the 1st quarter payroll related to disaster may provide some clarity on what those loans would have been influenced by. This could also help to take into account some seasonality in payroll flows. That said, since we have annual payroll included already this is a less important variable to consider.

- Sales or Revenue: This variable is important particularly for SBA content and EIDL loans where the firm needs to prove a certain amount of economic injury. [A survery](https://www.fedsmallbusiness.org/survey/2018/report-on-disaster-affected-firms) by the Federal Reserve board found that in disaster-related lending that forgone revenues (not assets) were the largest source of loss. Moreover, for PPP purposes, this is the calculation required for companies with no employees or independent contractors.

**North American Industry Classification System (NAICS) Limitations**

One of the challenges related to the census level data requests from the Census API is the need to specify NAICS codes to avoid crashing our systems. As such, we created a function that specifies the NAICS codes for the top 10 industries who received PPP funding as of April 13th, 2020. Those industries are: 

- 23: Construction 
- 31-33: Manufacturing:
- 42: Wholesale
- 44-45: Retail Trade
- 53: Real Estate Rental and Leasing
- 54: Professional, Scientific and Technical Services
- 56: Administrative and Support and Waste Management and Remediation Services
- 62: Health Care and Social Assistance
- 72: Accommodation and Food Services 
- 81: Other Services (except Public Administration)

While the rank of these industries is somewhat suspect when considering which industries are most likely to be [impacted by the crisis](https://www.brookings.edu/research/how-local-leaders-can-stave-off-a-small-business-collapse-from-covid-19/), they cover the majority of industries which would most likely to be impacted by the crisis.

We should also note that for the 2017 survery Construction, Manufacturing, and Wholesale retail were not available.

```{r reading and merging in Economic Census data from Census API, warning = F}
#2012 and 2007 include all the relevent NAICS data so will use for both dataframes
NAICS0712 <- c("23", "31-33", "42", "44-45", "53", "54", "56", "62", "72", "81")

sba_census_2007 <- map_df(.x = NAICS0712,
                          .f = sba_api_2007)
#Noting there is NO PAYQRT1 variable available for 2007.

#run for each year and NAICS code and the merge into three dataframes:
sba_census_2012 <- map_df(.x = NAICS0712,
                          .f = sba_api_2012)

NAICS2017 <- c("44-45", "53", "54", "56", "62", "72", "81")
#CENSUS for 2017 doesn't have Construction, Manufacturing, Whole sale retailers available.

sba_census_2017 <- map_df(.x = NAICS2017,
                          .f = sba_api_2017)

sba_census_merged <- bind_rows(sba_census_2007, sba_census_2012, sba_census_2017)
```
 
To refine the census data further, we include our targeted predictor variables from the census surveys taken in 2007, 2012, and 2017 and average them such that we create a 10-year average for each variable. We should note that there is no first quarter payroll included in 2007 data.

```{r cleaning SBA census data and averaging predictor variables}
sba_census_merged <- sba_census_merged %>%
  mutate(EMP = as.double(EMP),
         ESTAB = as.double(EMP),
         PAYANN = as.double(PAYANN),
         RCPTOT = as.double(RCPTOT),
         PAYQTR1 = as.double(PAYQTR1)) %>%
  group_by(state, county, GEO_ID, NAICS2012) %>%
  summarize(
    emp = mean(EMP),
    estab = mean(ESTAB),
    payann = mean(PAYANN),
    rcptot = mean(RCPTOT),
    payqrt1 = mean(PAYQTR1))
 
sba_census_merged <- sba_census_merged%>%
  rename(countyFIPS = county,
         stateFIPS = state)
```

**COVID-19 Cases and Deaths**

To further tie the prediction to the COVID crisis we also include data provided by the New York Times on their public [GitHub page](https://github.com/nytimes/covid-19-data), which includes the count of COVID cases and deaths by county. We have to manipulate the data somewhat to total the data over time to get an accurate total per county, which we then merge with our census data.

```{r reading in COVID cases/deaths}
#Adding in COVID cases/deaths 5.3.20:
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")

covid <- covid %>%
  mutate(stateFIPS = substr(fips, 1, 2), 
         countyFIPS = substr(fips, 3, 5))

covid <- covid %>%
  group_by(stateFIPS, countyFIPS) %>%
  summarise(covid_deaths = sum(deaths),
            covid_cases = sum(cases))
```

```{r merging COVID and Census Data into single dataframe, echo = F}
#merging onto sba_census data:
predictors <- sba_census_merged %>%
  left_join(covid, by = c("stateFIPS", "countyFIPS"))
```

**Michael Bay Effect**

We are also adding in an additional indicator variable to demarcate unusually significant natural disasters, which we are delineating as the top 1% of disasters. This includes storms such as Hurricane Sandy (2012), Hurricane Maria (2017), and Hurricane Harvey (2017) - all of which cause unusually high amounts of damage. All other disasters will receive a value equal to zero. It is our hope that during the model training process we may be able to amplify the impact of a disaster, such that we can show what the cost might be were a major disaster such as Sandy, Maria, or Harvey to hit the entire country like the COVID virus has.

```{r Micheal Bay Effect Variable}
#adding column for major disaster variable:
agg_disasters <- sba_merged_all %>%
  group_by(`SBA Disaster Number`) %>%
  summarise(sum(`Total Approved Loan Amount`)) %>%
  drop_na()

names(agg_disasters)[2] <- "Total_Loans"

Baysian_Disasters <- agg_disasters %>%
  filter(Total_Loans > quantile(Total_Loans, 0.99))
#Provides the disasters in the highest 1%, which are:
#"TX-00487", "FL-00130", "NY-00130", "PR-00031", "PR-00031", "	NJ-00033"

#Adding variable for top 1% of disasters
x = c("TX-00487", "FL-00130", "NY-00130", "PR-00031", "PR-00031", "	NJ-00033")
sba_merged_all <- sba_merged_all %>%
  mutate(
MBay_effect = ifelse(sba_merged_all$`SBA Disaster Number`%in% x, 1 ,0))
```

With all that done we are then able to merge our SBA disaster loan data with our combined COVID/Census data to create a final data frame for machine learning analysis.

```{r merging census and SBA disaster datasets into single dataframe}
#merging county and disaster numbers:
sba_merged_final <- sba_merged_all %>% 
  left_join(predictors, by = c("countyFIPS" = "countyFIPS",
                               "stateFIPS" = "stateFIPS"))
```

### Exploring the Data Further:

Now that there is a centralized dataset including SBA disaster loan totals and economic census data from counties, we will test for the relationships between the variables to help configure the model formulas we plan to use during the machine learning training and testing later. To reiterate or key dependence variables for consideration are avg/total county EDIL loans and avg/total county content loan amounts. We would like to narrow these to either EIDL or county content to minimize the number of potential models for consideration. 

First, we want to check out the correlation between the variables:

**Correlation Between Variables**

```{r checking out correlation, echo = F, warning = F}
#Finding correlation
sba_corr <- sba_merged_final %>%
  select(total_county_eidl_loan, avg_county_eidl_loan, total_county_approved_content_loan, avg_county_approved_content_loan, total_county_verified_content_loan, avg_county_verified_content_loan, MBay_effect, emp, payann, rcptot, payqrt1, estab, covid_cases, covid_deaths)

names_corr <- c("total_county_eidl_loan", "avg_county_eidl_loan", "total_county_approved_content_loan", "avg_county_approved_content_loan", "total_county_verified_content_loan", "avg_county_verified_content_loan", "MBay_effect", "emp", "payann", "rcptot", "payqrt1", "estab", "covid_cases", "covid_deaths")

sba_corr <- as.tibble(cor(na.omit(sba_corr)))

sba_corr <- sba_corr %>%
  mutate(names = names_corr) %>%
  select(names, total_county_eidl_loan, avg_county_eidl_loan, total_county_approved_content_loan, avg_county_approved_content_loan)
```

```{r correlations}
sba_corr
```

As we can see from the correlation output there are strong relationships with our predictor variables. At first blush it looks like our averaged dependents might have better connections than our totals.

**Choice Between county approved content loans and EIDL loans**

As mentioned earlier - we're looking to limit our formulas to two dependent variables to limit the complexity of the models later. There are promising reasons for each variable, but it would be worth checking the data plots.

```{r checking out dependent variable, echo = F}
#total_county_approved_content_loan
p1 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_approved_content_loan))) +
  geom_histogram() +
  my_theme +
  labs(x = "total content loan")

#avg_county_approved_content_loan
p2 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_approved_content_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. content loan")

#total_county_eidl_loan
p3 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "total EIDL loan")

#avg_county_eidl_loan
p4 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. EIDL loan")

```

```{r dependent grid, warning = F}
grid.arrange(p1, p2, p3, p4, nrow = 2)

#TOP:
#P1 = total_county_approved_content_loan
#P2 = avg_county_approved_content_loan

#BOTTOM:
#P3 = total_county_eidl_loan
#P4 = avg_county_eidl_loan
```

All plots transformed with `log()`

Again, as the plots indicate, the averages might prove to be a better predictor than our totals given the distribution. While we will not use a `log()` on the dependent it's clear the distributions are cleaner.

As a result - we are choosing to go with EIDL loans. This is for a couple reasons. First, we believe the EIDL loan mandates are more consistent with the requirements for the Paycheck Protection Program in that firms need to apply for [specific economic injury](https://www.sba.gov/funding-programs/loans/coronavirus-relief-options/economic-injury-disaster-loan-emergency-advance) related to the disaster. Similarly, PPP is economic injury as opposed to physical damage. 

There is a downside for choosing EIDL loans over content. Specifically, we will likely be underestimating the total amount due to the stringency of the program under normal cirsumtances compared to PPP. Currently, the SBA has loosened firm eligibility making it easier for firms to get lending and larger loans. This will be a potential liability in our model.

**Exploration of Predictor Variables**

We also wanted to quickly check on the distribution of the predictor variables and whether or not there is a need to create pre-processing methods for our model fitting.

```{r checking out independent variables, echo = F}
#number of employees:
p7 <- ggplot(sba_merged_final, mapping = aes(x = log(emp))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#number of establisments:
p8 <- ggplot(sba_merged_final, mapping = aes(x = log(estab))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#annual payroll:
p9 <- ggplot(sba_merged_final, mapping = aes(x = log(payann))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#payroll in the first quarter:
p10 <- ggplot(sba_merged_final, mapping = aes(x = log(payqrt1))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#sales or revenue:
p11 <- ggplot(sba_merged_final, mapping = aes(x = log(rcptot))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#Micheal Bay effect:
p12 <- ggplot(sba_merged_final, mapping = aes(x = MBay_effect)) +
  geom_histogram(binwidth = 0.35) +
  my_theme
```

```{r independent grid, warning = F}
grid.arrange(p7, p8, p9, p10, p11, p12, nrow = 2)

#TOP:
#P7 = number of employees:
#P8 = number of establisments:
#P9 = annual payroll:

#BOTTOM:
#P10 = payroll in the first quarte
#P11 = sales or revenue
#P12 = Micheal Bay effect
```

All the plots created above included `log()` transformations. It is clear we will need additional preprocessing steps to also include this transformation. 

## Model Specifications

### Algorithm Considerations

We think it would be good to have a couple options to help see which might help the model fit better. Specifically:

- CART Decision Tree: We believe this will likely be our best algorithm for the complexity of the models we are using. We also think it provides the best ease of understanding for our audience. While there are many versions of tree-based algorithms, we think the [Classification and Regression Tree (CART)](https://bradleyboehmke.github.io/HOML/DT.html), makes the most sense since many of our variables are numeric and discreate. Since we are also using total and averaged loan sizes across county, the basic function of how trees create subgroups via an averaged response should make the algorithm more accurate for our dataset. 

- Linear Regression: We also felt it was important to include a baseline algorithm like linear regression. First, it's easy to understand when explaining our findings. That said, there are likely downsides to this model. It is likely our data is not linear in any form. Moreover, we would likely need to include further dummy variables to account for the non-linearity. Nonetheless, we think it's important to include if for no other reason than an easily explainable baseline.

- Random Forests: We also wanted to explore a more robust version of decision trees, with a random forest algorithm. We understand this algorithm should be [easy to use out-of-the-box](https://bradleyboehmke.github.io/HOML/random-forest.html), but we need to further understand some of our missing values and their relationship to the data since the algorithm won't work with missing values.

**Testing Two Models**

After exploring the data further, we have two different potential model formulas to test:

- Average County EIDL Loan as a function of: average county verified content loan, employees, annual payroll, total receipts, payroll in the first quarter, number of establishments in its jurisdiction, cases of COVID-19, and deaths from COVID-19. We are assuming that this will be a top 1% disaster, triggering the Michael Bay effect. [avg_county_eidl_loan ~ avg_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect]

- Total County EIDL Loan as a function of total county verified content loan, employees, annual payroll, total receipts, payroll in the first quarter, number of establishments in its jurisdiction, cases of COVID-19, and deaths from COVID-19. We are assuming that this will be a top 1% disaster, triggering the Michael Bay effect. [total_county_eidl_loan ~ total_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect]

**Missing Values**

As mentioned earlier, it is important to consider the missing values in our data set. This is important since the random forest algorithm will require it but also how NAs will impact on our other models. We will quickly subset our data to the variables of interest and create a sample of 50,000 observations so running tests is manageable.

```{r checking on missing values}
#to get a subset for missing visualizations
sba_merged_model <- sba_merged_final %>%
  select(avg_county_eidl_loan, total_county_eidl_loan, total_county_verified_content_loan, 
         payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  sample_n(50000) 
```

```{r visualizing missing data across variables}
vis_miss(sba_merged_model) #to see NAs across the variable spectrum
```

It looks like we only have significant missing values in our first quarter payroll as well as in our covid cases and covid deaths.

```{r visualizing relationship between missing values}
gg_miss_upset(sba_merged_model) #checking about relationship between the variables
```

As the earlier graph indicated a lot of the data's missing values are within the covid-related data and the payroll for the first quarter . While there is some relationship across the missing values, we feel confident it isn't going to greatly impact the estimations of the models nor remove incredibly useful information from our estimations since it wasn't an available prodcut of Census in 2007.

**Pre-Processing**

- As mentioned earlier most of our dependent variable will be transformed with a `step_log()` command. these include our number of employees, annual payroll, number of estabilishment, payroll for the first quarter, and the firm’s revenue and sales. COVID cases and deaths as well as our Michael Bay factor will not be transormed. We have also included a command to offset the values due numerous observations in all variables equal to zero.
  
```{r pre-process for models}
#pre-process for CART and linear:
sba_merged_model <- sba_merged_final %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, avg_county_approved_content_loan, total_county_approved_content_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(sba_merged_model) %>% #omitting missing values
  sample_n(50000) #subsetting the dataset for training and testing

set.seed(seed = 20200428)

split <- initial_split(sba_merged_model, prop = 0.80)
SBA_train <- training(split)
SBA_test <- testing(split)
SBA_resamples <- vfold_cv(data = SBA_train,
                          v = 10)
```

**Error Term**

When comparing models, we elected to use Root Mean Squared Error (RMSE) to estimate model error. RMSE was chosen because of its flexibility for both CART and linear regression. RMSE also weighs outliers more heavily, and since this economic crisis is, by definition, an outlier - we think it is important to base our model's estimations on an error term more influenced by outlier observations.

### Testing Models in Resampling for CART and Linear algorithms:

We will be testing four specific models for each of the two formulas and each of the two algorithms such that we will have:
- Averaged CART
- Total CART
- Averaged Linear
- Total Linear

```{r linear and CART regression algorithm, warning = F}
#Comparing RMSE across our two formulas and two algorithms:
SBA_resamples <- SBA_resamples %>%
  mutate(
    rmse_CARTavg = map_dbl(.x = splits,
                         .f = ~model_CART_avg(split = .x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_linearavg = map_dbl(.x = splits,
                           .f = ~model_linear_avg(.x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_CARTtotal = map_dbl(.x = splits,
                         .f = ~model_CART_total(split = .x,
                                          formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                            emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                            covid_deaths + MBay_effect)),
    rmse_lineartotal = map_dbl(.x = splits,
                           .f = ~model_linear_total(.x,
                                              formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                                emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                                covid_deaths + MBay_effect)))
```

We will first want to look across the ten folds:

```{r across resamples}
SBA_resamples %>%
  select(id, rmse_CARTavg, rmse_linearavg, rmse_CARTtotal, rmse_lineartotal)
```

As well as look at the mean of the resamples:

```{r mean of resamples}
SBA_resamples %>%
  summarize(mean(rmse_linearavg),
            mean(rmse_CARTavg),
            mean(rmse_CARTtotal),
            mean(rmse_lineartotal))
```

It's clear from this model that our **Averaged CART** does the best job among the four model options. This tracks with our intuition: this are a complex model with many variables and is more naturally suited to a decision tree model than a simple linear model. Taking this logic one step further we attempted to run a random forests model to further reduce RMSE. 

**Random Forest Test**

We wanted to try our hand at a random forest algorithm but couldn't process it across folds in a resample so supplied two instances against the trainingt dataset.

```{r RandomForest algorithm for avg_county_eidl_loan}
n_features <- length(setdiff(names(SBA_train), "avg_county_eidl_loan"))

sba_rf_avg <- ranger(
  formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train,
  num.tree = n_features*10,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(avg_rmse <- sqrt(sba_rf_avg$prediction.error))
```

We can see that across the training set, that our averaged random forest does better than most.

```{r RandomForest algorithm for total_county_eidl_loan}
n_features1 <- length(setdiff(names(SBA_train), "total_county_eidl_loan"))

sba_rf_total <- ranger(
  formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train,
  num.tree = n_features1*10,
  mtry = floor(n_features1 / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(total_rmse <- sqrt(sba_rf_total$prediction.error))
```

It also appears that random forest for total does better compared to other totals across the training data.

The random forests model decreased the RMSE and produced a better model than CART. However, due to our computational limitations, we were not able to explore random forests further as part of this analysis, which as you will see is an area of improvement we think very worthwhile.

Hence, we elected to go with average county EIDL loan via CART as our preferred final model for testing.

``` {r Final Model}
#remove NA:
SBA_train <- SBA_train %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, avg_county_approved_content_loan, total_county_approved_content_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(SBA_train)

#create recipe:
final_recipe<- 
    recipe(formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
           data = SBA_train) %>%
    step_log(total_county_verified_content_loan, offset = 0.00001) %>%
    step_log(emp, offset = 0.00001) %>%
    step_log(payann, offset = 0.00001) %>%
    step_log(rcptot, offset = 0.00001) %>%
    step_log(payqrt1, offset = 0.00001) %>% 
    step_log(estab, offset = 0.00001) %>% 
    prep()

#create model:
final_model <- decision_tree(mode = "regression") %>%
    set_engine("rpart") %>%
    fit(formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect, 
    data = SBA_train)

#test model:
final_rmse <- bind_cols(
  bake(final_recipe, new_data = SBA_test),
  pred = predict(final_model, bake(final_recipe, new_data = SBA_test)) 
  )%>%
  rmse(truth = avg_county_eidl_loan, estimate = .pred) %>%
  pull(.estimate)


final_rmse
```

As we can see from the final output there has been a slight uptick in the RMSE when running our model through our testing data. Our estimate becomes approximately $12,000 less accurate in predicting what EIDL loan sizes would be from natural disasters. We believe this isn't a terrible conclusion but believe it clearly points to areas of improvement, which we will discuss later.

**Bias - Variance Tradeoff**
We believe it is important to note at this point that our current model is likely of low-bias and high variance, which is probably an okay place to arrive given the algorithm of choice. Again, we will discuss areas of improvement later.

## Applying Model to Current Crisis:

**Unfortunately Don't Have Current Data:**

As of this writing, the SBA has yet to release data related to the PPP. It was hoped that as we began this project it would be available but we're still in the waiting phase. Once that data is available, we can compare that against our model to see how effective it would have been.

**Still Upsides to the Model**

Having a model like this available to policymakers as they craft policy with as much as uncertainty as PPP would enable better policy outcomes and proper funding of programs, eliminating situations similar to the one faced at the outset of this program. Moreover, we believe the creation of the Michael Bay effect could help to engineer a stronger prediction in future data, since we can in short tweak the variable to anticipate how much of an outlier the coronavirus pandemic has been. In short, we could try to predict what loan sizes would be had Harvey, Maria, or Sandy hit the entire country as opposed to specific states.

## Areas of Improvement:

We believe the first attempt at the model was sufficient but as noted there are areas where it can be great improved. First would be to consider a more aggregated variable for prediction. While EIDL were more aligned by need, as a loan, underestimate the need and account for a smaller portion of natural disaster funding. Content Loans could have perhaps proven a better fit and would be the first place to start given the Federal Reserve has indicated that [40% of 7(a) SBA disaster funding](https://www.fedsmallbusiness.org/survey/2018/report-on-disaster-affected-firms) is accounted by revenue loss and not physical property damage. 

Moreover, there are likely errors in the data wrangling which could further account for less accurate data. With more time, accounting for those errors would make for a more accurate model. The same goes for handling missing information. Instead of omitting the data, we could go back and impute the missing data with machine learning models, although that would require significant more time and thought. 

Lastly, related to our attempt to balance or model's bias - variance trade off, we could further understand other algorithm options - like we tried with random forests - and further tune and dial the parameters to understand the optimal arrangement for nodes, and subgroups in our decision tree models and other confounding variables in our linear regression - we are sure we are missing some dummy variables that are important. It is important to note, that while we recognize the need for further balancing, given the implicit challenge at predicting current data on historical that balancing challenge will always exist.

Nonetheless, we feel proud of the work presented here and hope that the SBA will release PPP data soon, thus allowing us to really see how well our model performed.
