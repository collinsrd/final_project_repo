---
title: "Final Project"
author: "Ryan Collins and Alex Clegg"
date: "4/18/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

Here are the packages that will be needed for this assignment:

```{r packages}
library(readr)
library(readxl)
library(tidyverse)
library(tidymodels)
library(httr)
library(jsonlite)
library(randomForest)
source("R/header.true.R")
source("R/sba_api_2012.R")
source("R/sba_api_2017.R")
source("R/sba_api_2007.R")
source("R/add_year.R")
```

Working on reading in the dataset and merging the datasets together.
```{r reading and merging SBA disaster loan data, warning = F}
#first for the disaster loan-size:
sba_sandy <- read_excel("data/SBA_Disaster_Loan_Data_Superstorm_Sandy_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))

#adding column for major disaster variable
sba_sandy <- sba_sandy %>%
  mutate(MBay_effect = 1)

sba_2008 <- read_excel("data/SBA_Disaster_Loan_Data_FY08_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2009 <- read_excel("data/SBA_Disaster_Loan_Data_FY09_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2010 <- read_excel("data/SBA_Disaster_Loan_Data_FY10_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2011 <- read_excel("data/SBA_Disaster_Loan_Data_FY11_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2012 <- read_excel("data/SBA_Disaster_Loan_Data_FY12_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2013 <- read_excel("data/SBA_Disaster_Loan_Data_FY13_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2014 <- read_excel("data/SBA_Disaster_Loan_Data_FY14_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2015 <- read_excel("data/SBA_Disaster_Loan_Data_FY15_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2016 <- read_excel("data/SBA_Disaster_Loan_Data_FY16_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2017 <- read_excel("data/SBA_Disaster_Loan_Data_FY17_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))

#adding column for major disaster variable:
x = c("FL", "TX", "PR", "VI")
sba_2017 <- sba_2017 %>%
  mutate(
MBay_effect = ifelse(sba_2017$`Damaged Property State Code`%in% x, 1 ,0))

sba_2018 <- read_excel("data/SBA_Disaster_Loan_Data_FY18_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))

#adding year variable:
years <- c(2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018)

sba_2008 <- add_year(sba_2008, 1)
sba_2009 <- add_year(sba_2009, 2)
sba_2010 <- add_year(sba_2010, 3)
sba_2011 <- add_year(sba_2011, 4)
sba_2012 <- add_year(sba_2012, 5)
sba_sandy <- add_year(sba_sandy, 5)
sba_2013 <- add_year(sba_2013, 6)
sba_2014 <- add_year(sba_2014, 7)
sba_2015 <- add_year(sba_2015, 8)
sba_2016 <- add_year(sba_2016, 9)
sba_2017 <- add_year(sba_2017, 10)
sba_2018 <- add_year(sba_2018, 11)


#binding columns:
sba_merged_all<- bind_rows(sba_sandy, sba_2008, sba_2009, sba_2010, sba_2011, sba_2012, sba_2013,
                           sba_2014, sba_2015, sba_2016, sba_2017, sba_2018)

#fill in the remainder of sandy variable:
sba_merged_all$MBay_effect[is.na(sba_merged_all$MBay_effect)] <- 0

#renaming zipcode:
sba_merged_all <- sba_merged_all %>%
  rename(zipcode = `Damaged Property Zip Code`)

#Now will wan to read in data for fips codes:
ZIP_COUNTY_FIPS_2012_12 <- read_csv("data/ZIP-COUNTY-FIPS_2012-12.csv")

#changing names for target variable (zipcode)
ZIP_COUNTY_FIPS_2012_12 <- ZIP_COUNTY_FIPS_2012_12 %>%
  rename(zipcode = ZIP) %>%
  select(zipcode, STCOUNTYFP)

#merging on FIPS Codes:
sba_merged_all <- sba_merged_all %>% 
  left_join(ZIP_COUNTY_FIPS_2012_12, by = "zipcode")

#now to separate out the state and county fips codes:
sba_merged_all <- sba_merged_all %>%
  mutate(stateFIPS = substr(STCOUNTYFP, 1, 2), 
         countyFIPS = substr(STCOUNTYFP, 3, 5))

#creating weight for countys with multiple zipcodes:
sba_agg <- sba_merged_all %>% 
  group_by(zipcode, year) %>%
  summarise(n = n())

sba_merged_all <- left_join(sba_merged_all, 
                             sba_agg,
                             by = c("zipcode", "year"))

sba_merged_all <- sba_merged_all %>%
  mutate(weighted_content_loan = `Approved Amount Content`/n,
         weighted_loss_content = `Verified Loss Content`/n) 

sba_weighted <- sba_merged_all %>%
  group_by(countyFIPS, stateFIPS, year) %>%
  summarize(
    total_county_approved_content_loan = sum(weighted_content_loan),
    total_county_verified_content_loan = sum(weighted_loss_content),
    avg_county_approved_content_loan = weighted.mean(weighted_content_loan, weight = 1/n),
    avg_county_verified_content_loan = weighted.mean(weighted_loss_content, weight = 1/n),
  )

#now merging them all onto sba_merged 
sba_merged_all <- left_join(sba_merged_all, 
                             sba_weighted,
                             by = c("countyFIPS", "stateFIPS", "year")) 

sba_merged_all <- sba_merged_all%>%
  select(year, zipcode, countyFIPS, stateFIPS, `Total Verified Loss`, `Verified Loss Content`, `Total Approved Loan Amount`, `Approved Amount Content`, `Approved Amount EIDL`, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan, MBay_effect, `SBA Disaster Number`)
```

For reading in county level data I'll create a function that allows input of NAICS codes for the top 10 industries provided PPP funding as of April 13th, 2020. (Although we can change this depending on the depth and detial of the industries we'd like to focus on. Currently we are looking at):

- 23: Consturction 
- 31-33: Manufacturing:
- 42: Whole Sale
- 44-45:Rtail Trade
- 53: Real Estate Rental and Leasing
- 54: Professional, Scientific and Technical Services
- 56: Administrative and Support and Waste Management and Remediation Services
- 62: Health Care and Social Assistance
- 72: Accomodation and Food Services [WE MAY WANT TO CONSIDER MORE HERE]
- 81: Other Services (expcept Public Administration)

```{r merging in Economic Census data from Census API, waring = F}
#2012 and 2007 include all the relevent NAICS data so will use for both dataframes
NAICS0712 <- c("23", "31-33", "42", "44-45", "53", "54", "56", "62", "72", "81")

sba_census_2007 <- map_df(.x = NAICS0712,
                          .f = sba_api_2007)
#Noting there is NO PAYQRT1 variable available for 2007.

#run for each year and NAICS code and the merge into three dataframes:
sba_census_2012 <- map_df(.x = NAICS0712,
                          .f = sba_api_2012)

NAICS2017 <- c("44-45", "53", "54", "56", "62", "72", "81")
#CENSUS for 2017 doesn't have Construction, Manufacturing, Whole sale retailers available.

sba_census_2017 <- map_df(.x = NAICS2017,
                          .f = sba_api_2017)

sba_census_merged <- bind_rows(sba_census_2007, sba_census_2012, sba_census_2017)

sba_census_merged <- sba_census_merged %>%
  mutate(EMP = as.double(EMP),
         ESTAB = as.double(EMP),
         PAYANN = as.double(PAYANN),
         RCPTOT = as.double(RCPTOT),
         PAYQTR1 = as.double(PAYQTR1)) %>%
  group_by(state, county, GEO_ID, NAICS2012) %>%
  summarize(
    emp = mean(EMP),
    estab = mean(ESTAB),
    payann = mean(PAYANN),
    rcptot = mean(RCPTOT),
    payqrt1 = mean(PAYQTR1))
 
sba_census_merged <- sba_census_merged%>%
  rename(countyFIPS = county,
         stateFIPS = state)

#Adding in COVID cases/deaths 5.3.20:
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")

covid <- covid %>%
  mutate(stateFIPS = substr(fips, 1, 2), 
         countyFIPS = substr(fips, 3, 5))

covid <- covid %>%
  group_by(stateFIPS, countyFIPS) %>%
  summarise(covid_deaths = sum(deaths),
            covid_cases = sum(cases))

#merging onto sba_census data:
predictors <- sba_census_merged %>%
  left_join(covid, by = c("stateFIPS", "countyFIPS"))

```

```{r merging census and SBA disaster data sets}
#merging county and disaster numbers:
sba_merged_final <- sba_merged_all %>% 
  left_join(predictors, by = c("countyFIPS" = "countyFIPS",
                               "stateFIPS" = "stateFIPS"))

#NOT SURE IF WE NEED TO DO THIS.
#cleaning out the duplicates:
  
#sba_merged_final <- sba_merged_final %>%
#  mutate(duplicate = duplicated(sba_merged_final$`Total Verified Loss`)) %>%
#  filter(duplicate == !(TRUE)) %>%
#  select(-duplicate)
```

```{r quick check of the finalized dataset and variables}
summary(sba_merged_final)
```

```{r checking out the data,  warning = F}
ggplot(sba_merged_final, mapping = aes(x = rcptot, y = `Total Approved Loan Amount`)) +
  geom_point() +
  geom_smooth() 

ggplot(sba_merged_final, mapping = aes(x = payann, y = `Approved Amount Content`)) +
  geom_point() +
  geom_smooth() 

ggplot(sba_merged_final, mapping = aes(x = `Verified Loss Content`, y = `Approved Amount Content`)) +
  geom_point() +
  geom_smooth() 

ggplot(sba_merged_final, mapping = aes(x = log(`Approved Amount Content`))) +
  geom_histogram() 
  
ggplot(sba_merged_final, mapping = aes(x = MBay_effect)) +
  geom_histogram()

#Finding correlation

sba_corr <- sba_merged_final %>%
  select(`Total Verified Loss`, `Verified Loss Content`, `Total Approved Loan Amount`, `Approved Amount Content`, `Approved Amount EIDL`, covid_cases, covid_deaths, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan, MBay_effect, emp, payann, rcptot, payqrt1)

cor(na.omit(sba_corr))
```


```{r Creating MBay}
#We can take or leave this, but thought it'd be interesting to have a formal way to classify the huge disasters

agg_disasters <- sba_merged_final %>%
  group_by(`SBA Disaster Number`) %>%
  summarise(sum(`Total Approved Loan Amount`)) %>%
  drop_na()

names(agg_disasters)[2] <- "Total_Loans"

Baysian_Disasters <- agg_disasters %>%
  filter(Total_Loans > quantile(Total_Loans, 0.99))
```

## Model specifications (DRAFT)

**Variables under consideration**

Dependent Variable:

We have three potential variables for consideration: 

- Total loan approved, 
- Content loan approved 
- EIDL approved amount.

Our hunch is to lean towards the approved content loan as it would remove the real estate component from total loan amounts, since PPP loans are technically eligible for 250% of the last yeart of payroll. Moreover, while EIDL is a component of the SBA Paycheck protection program it's eligbility is more stringent which would mean that we could underestimate the prective amount.

Independent Variables: 

- The Micahel Bay factor:

Because of high correlation with larger loan sizes and allows for us to engineer a scenario where all businesses are experiencing a major disaster.
  
- Business components:

As it stands now, we have NAICS codes based upon the top 10 sectors receiving PPP as of a couple of weeks ago.Although this group can be expanded via the API in our code above request to go deeper and/or broader depending on if we want to highlight additional components. For example a recent Brookings[LINK] report break down industries into three different categories of risk, which could be an interesting additional indicator variable of at-risk industries that, like the Mbay factor, could give us some control about the size of relief.

The other business component have been pulled from the Economic Census (as part of the U.S. Census), which is the official measure of the Nationâ€™s businesses and economy. Conducted every five years, the survey serves as the statistical benchmark for current economic activity. The variables of interest include averages over 2007, 2012, and 2017 for:

  - employee numnber, 
  - number of establishments in the jurisdiction, 
  - payroll,
  - payroll of the first quarter,
  - Sales and revenue value.

Beyond these variables we are in further consideration on whether we want to specify size of the business by employee due to the eligibility requirements that no buisness over the size of 500 should be eligible. However, there are numerous reports indicating that firms with larger workforces have recieved loans. Therefore, it is important to note that for the purposes of this predictive model that we are [or are not] including businesses over 500 employees.

Additionally, we are consdering the inculsion of further indicator variables to catagorize size of buisness for further identification of micro-businesses, which are more illiquid. It may be important to find a weighted variable to further capture the importance of business size. 

We will also be including verified loss by the SBA during disaster analysis. There is a strong corelation between the approved amount and total verfied loss. We will need to explain why is considered in verified loss.
Important to include because of correlation 

Additional potential variables:

- Geographic variable: Would it make sense to include a population density component for the data as well given that urban/rural distributions have an impact on buisness as well as potential virus impacts.

- Time variable: Would we want to think through the ramifications on the size of loan if the crisis goes on longer than expected? If so, how do you think we could model this?

**Algorithm Considerations: **

We think it would be good to have a couple options to help see which might help the model fit better. Specifically:

- Random Forests
- Classic CART option
- Linear Regression 


``` {r 3 Methods}
set.seed(seed = 20200428)
split <- initial_split(XXX, prop = 0.80)
SBA_train <- training(split)
SBA_test <- testing(split)
SBA_resamples <- vfold_cv(data = SBA_train,
                          v = 10)

#unsure what the code for random forests would be

SBA_RF <- function(split, formula,...) {
  recipe <- 
    recipe(formula, data = analysis(split)) %>%
    log vars %>%
    prep()
  analysis_data <- analysis(split) %>%
    bake(object = recipe, new_data = .)
  model <- linear_reg() %>%
    set_engine(???) %>%
    fit(formula, data = analysis_data)
  assessment_data <- assessment(split) %>%
    bake(object = recipe, new_data = .)
  rmse <- bind_cols(
    assessment_data,
    predict(model, assessment_data)
  ) %>%
    rmse(truth = XXX, estimate = .pred) %>%
    pull(.estimate)
  
  return(rmse)
}

SBA_CART <- function(split, formula,...) {
  recipe <- 
    recipe(formula, data = analysis(split)) %>%
    log vars %>%
    prep()
  analysis_data <- analysis(split) %>%
    bake(object = recipe, new_data = .)
  model <- decision_tree(mode = "regression") %>%
    set_engine("rpart") %>%
    fit(formula, data = analysis_data)
  assessment_data <- assessment(split) %>%
    bake(object = recipe, new_data = .)
  rmse <- bind_cols(
    assessment_data,
    predict(model, assessment_data)
  ) %>%
    rmse(truth = XXX, estimate = .pred) %>%
    pull(.estimate)
  
  return(rmse)
}

SBA_linear <- function(split, formula,...) {
  recipe <- 
    recipe(formula, data = analysis(split)) %>%
    log vars %>%
    prep()
  analysis_data <- analysis(split) %>%
    bake(object = recipe, new_data = .)
  model <- linear_reg() %>%
    set_engine("lm") %>%
    fit(formula, data = analysis_data)
  assessment_data <- assessment(split) %>%
    bake(object = recipe, new_data = .)
  rmse <- bind_cols(
    assessment_data,
    predict(model, assessment_data)
  ) %>%
    rmse(truth = XXX, estimate = .pred) %>%
    pull(.estimate)
  
  return(rmse)
}



```

``` {r Choose Model}

SBA_resamples <-
  mutate(
    rmse1 = map_dbl(splits,
                    ~SBA_CART(split = .x,
                              formula = ??????)),
    rmse2 = map_dbl(splits,
                    ~SBA_RF(split = .x,
                              formula = ??????)),
    rmse3 = map_dbl(splits,
                    ~SBA_linear(split = .x,
                              formula = ??????)),
                    ))
  )


```

